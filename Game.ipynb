{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4ffd9d",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c449ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.8.0rc1, 2.8.0, 2.9.0rc0, 2.9.0rc1)\n",
      "ERROR: No matching distribution found for tensorflow==1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym) (1.22.3)\n",
      "Requirement already satisfied: keras in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: keras-rl2 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras-rl2) (2.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (4.2.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (2.8.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (2.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (1.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (58.1.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (1.22.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (0.25.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (1.16.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (14.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (1.44.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (0.5.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow->keras-rl2) (2.8.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.27.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.6.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.0.12)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
      "Requirement already satisfied: pygame in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: stable-baselines[mpi] in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.10.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (1.4.2)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (2.0.0)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (0.21.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (1.22.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (4.5.5.64)\n",
      "Requirement already satisfied: joblib in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (1.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (3.5.1)\n",
      "Requirement already satisfied: mpi4py in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines[mpi]) (3.1.3)\n",
      "Requirement already satisfied: ale-py~=0.7.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.7.5)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.23)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (3.0.8)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (9.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines[mpi]) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->stable-baselines[mpi]) (2022.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ale-py~=0.7.1->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (5.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oleg\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines[mpi]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.14\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2\n",
    "!pip install pygame\n",
    "!pip install stable-baselines[mpi]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956337d",
   "metadata": {},
   "source": [
    "# Test random environment from open AI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba53822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c231cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaresEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.number_of_shapes = 37\n",
    "        self.action_space = Box(low = 0, high = 1, shape=(3, ), dtype = np.float32)\n",
    "        self.observation_space = Box(low = 0, high = 1, shape=(84, ), dtype = np.float32)\n",
    "        \n",
    "        self.state = dict(\n",
    "        {\n",
    "            'board': np.zeros((9, 9), dtype = np.uint8),\n",
    "            'items': np.zeros(3, dtype = np.uint8)\n",
    "        })\n",
    "        self.time = 1\n",
    "        self.state['items'][0] = random.randint(1,self.number_of_shapes)\n",
    "        self.state['items'][1] = random.randint(1,self.number_of_shapes)\n",
    "        self.state['items'][2] = random.randint(1,self.number_of_shapes)\n",
    "        \n",
    "    def checkFull(self, minX, maxX, minY, maxY):\n",
    "        bingo = np.ones((3,3), dtype = np.uint8)\n",
    "        if np.array_equal(self.state['board'][minY:maxY, minX:maxX], bingo):\n",
    "            self.state['board'][minY:maxY, minX:maxX] = 0\n",
    "            print('yay')\n",
    "            return 10\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def getShape(self, shapeType):\n",
    "        shape = np.zeros((2,1))\n",
    "        if shapeType == 1: # X\n",
    "            shape = np.ones((1,1), dtype = np.uint8)\n",
    "        elif shapeType == 2: # XX\n",
    "            shape = np.ones((2,1), dtype = np.uint8)\n",
    "        elif shapeType == 3:\n",
    "            shape = np.ones((1,2), dtype = np.uint8)\n",
    "        elif shapeType == 4: # XXX\n",
    "            shape = np.ones((3,1), dtype = np.uint8)\n",
    "        elif shapeType == 5:\n",
    "            shape = np.ones((1,3), dtype = np.uint8)\n",
    "        elif shapeType == 6: # XXXX\n",
    "            shape = np.ones((4,1), dtype = np.uint8)\n",
    "        elif shapeType == 7:\n",
    "            shape = np.ones((1,4), dtype = np.uint8)\n",
    "        elif shapeType == 8: # square corner\n",
    "            shape = np.ones((2,2), dtype = np.uint8)\n",
    "            shape[0,0] = 0\n",
    "        elif shapeType == 9:\n",
    "            shape = np.ones((2,2), dtype = np.uint8)\n",
    "            shape[0,1] = 0\n",
    "        elif shapeType == 10:\n",
    "            shape = np.ones((2,2), dtype = np.uint8)\n",
    "            shape[1,0] = 0\n",
    "        elif shapeType == 11:\n",
    "            shape = np.ones((2,2), dtype = np.uint8)\n",
    "            shape[1,1] = 0\n",
    "        elif shapeType == 12: # square one sided long vertical\n",
    "            shape = np.ones((3,2), dtype = np.uint8)\n",
    "            shape[1:, 1] = 0\n",
    "        elif shapeType == 13:\n",
    "            shape = np.ones((3,2), dtype = np.uint8)\n",
    "            shape[1:, 0] = 0\n",
    "        elif shapeType == 14:\n",
    "            shape = np.ones((3,2), dtype = np.uint8)\n",
    "            shape[:2, 1] = 0\n",
    "        elif shapeType == 15:\n",
    "            shape = np.ones((3,2), dtype = np.uint8)\n",
    "            shape[:2, 0] = 0\n",
    "        elif shapeType == 16: # square one sided long horizontal\n",
    "            shape = np.ones((2,3), dtype = np.uint8)\n",
    "            shape[0, 1:] = 0\n",
    "        elif shapeType == 17:\n",
    "            shape = np.ones((2,3), dtype = np.uint8)\n",
    "            shape[0, :2] = 0\n",
    "        elif shapeType == 18:\n",
    "            shape = np.ones((2,3), dtype = np.uint8)\n",
    "            shape[1, 1:] = 0\n",
    "        elif shapeType == 19:\n",
    "            shape = np.ones((2,3), dtype = np.uint8)\n",
    "            shape[1, :2] = 0\n",
    "\n",
    "        elif shapeType == 20: # straight big corner\n",
    "            shape = np.ones((3,3), dtype = np.uint8)\n",
    "            shape[:2,1:] = 0\n",
    "        elif shapeType == 21:\n",
    "            shape = np.ones((3,3), dtype = np.uint8)\n",
    "            shape[:2,:2] = 0\n",
    "        elif shapeType == 22:\n",
    "            shape = np.ones((3,3), dtype = np.uint8)\n",
    "            shape[1:,1:] = 0\n",
    "        elif shapeType == 23:\n",
    "            shape = np.ones((3,3), dtype = np.uint8)\n",
    "            shape[1:,:2] = 0\n",
    "        elif shapeType == 24: # T shape\n",
    "            shape = np.ones((3,3), dtype = np.uint8)\n",
    "            shape[0,1:] = 0\n",
    "            shape[2,1:] = 0\n",
    "        elif shapeType == 25:\n",
    "            shape = np.ones((3,3), dtype = np.uint8)\n",
    "            shape[:2,0] = 0\n",
    "            shape[:2,2] = 0\n",
    "        elif shapeType == 26:\n",
    "            shape = np.ones((3,3), dtype = np.uint8)\n",
    "            shape[0,:2] = 0\n",
    "            shape[2,:2] = 0\n",
    "        elif shapeType == 27:\n",
    "            shape = np.ones((3,3), dtype = np.uint8)\n",
    "            shape[1:,0] = 0\n",
    "            shape[1:,2] = 0\n",
    "        elif shapeType == 28: #short T shape\n",
    "            shape = np.ones((3,2), dtype = np.uint8)\n",
    "            shape[0,1] = 0\n",
    "            shape[2,1] = 0\n",
    "        elif shapeType == 29:\n",
    "            shape = np.ones((3,2), dtype = np.uint8)\n",
    "            shape[0, 0] = 0\n",
    "            shape[2, 0] = 0\n",
    "        elif shapeType == 30:\n",
    "            shape = np.ones((2,3), dtype = np.uint8)\n",
    "            shape[1, 0] = 0\n",
    "            shape[1, 2] = 0\n",
    "        elif shapeType == 31:\n",
    "            shape = np.ones((2,3), dtype = np.uint8)\n",
    "            shape[0,0] = 0\n",
    "            shape[0,2] = 0\n",
    "        elif shapeType == 32: # S shape\n",
    "            shape = np.ones((2,3), dtype = np.uint8)\n",
    "            shape[0,0] = 0\n",
    "            shape[1,2] = 0\n",
    "        elif shapeType == 33:\n",
    "            shape = np.ones((2,3), dtype = np.uint8)\n",
    "            shape[1,0] = 0\n",
    "            shape[0,2] = 0\n",
    "        elif shapeType == 34:\n",
    "            shape = np.ones((3,2), dtype = np.uint8)\n",
    "            shape[0,1] = 0\n",
    "            shape[2,0] = 0\n",
    "        elif shapeType == 35:\n",
    "            shape = np.ones((3,2), dtype = np.uint8)\n",
    "            shape[0,0] = 0\n",
    "            shape[2,1] = 0\n",
    "        elif shapeType == 36: # square\n",
    "            shape = np.ones((2,2), dtype = np.uint8)\n",
    "        elif shapeType == 37: # cross \n",
    "            shape = np.zeros((3,3), dtype = np.uint8)\n",
    "            shape[:,1] = 1\n",
    "            shape[1,:] = 1\n",
    "        return shape\n",
    "    \n",
    "    def insertionPossible(self, shape, x, y):\n",
    "        shape_x = shape.shape[0]\n",
    "        shape_y = shape.shape[1]\n",
    "        if (x + shape_x) <= self.state['board'].shape[0] and (y + shape_y) <= self.state['board'].shape[1]:\n",
    "            #check collision\n",
    "            for local_x in range(0, shape_x):\n",
    "                for local_y in range(0, shape_y):\n",
    "                    if self.state['board'][x + local_x, y + local_y] == 1 and shape[local_x, local_y] == 1:\n",
    "                        return False\n",
    "        else:\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def thereAreOptions(self):\n",
    "        thereAreOptions = False\n",
    "        for shapeType in self.state['items']:\n",
    "            if shapeType != 0:\n",
    "                \n",
    "                shape = self.getShape(shapeType)\n",
    "                for x in range(0, self.state['board'].shape[0] - shape.shape[0]):\n",
    "                    for y in range(0, self.state['board'].shape[1] - shape.shape[1]):\n",
    "                        if self.insertionPossible(shape, x, y):\n",
    "                            thereAreOptions = True\n",
    "        return thereAreOptions\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.counter += 1\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        shapeIndex = round(np.clip(action[0], 0, 1) * 2)\n",
    "        x = round(np.clip(action[1], 0, 1) * 8)\n",
    "        y = round(np.clip(action[2], 0, 1) * 8)\n",
    "        \n",
    "\n",
    "        # if chosen shape is not yet used\n",
    "        if self.state['items'][shapeIndex] != 0:\n",
    "            shapeType = self.state['items'][shapeIndex]\n",
    "            self.state['items'][shapeIndex] = 0\n",
    "            shape = self.getShape(shapeType)\n",
    "            shape_x = shape.shape[0]\n",
    "            shape_y = shape.shape[1]\n",
    "            \n",
    "\n",
    "            if self.insertionPossible(shape, x, y):\n",
    "                reward += 3\n",
    "                for local_x in range(0, shape_x):\n",
    "                    for local_y in range(0, shape_y):\n",
    "                        self.state['board'][x + local_x, y + local_y] += shape[local_x, local_y]\n",
    "            else:\n",
    "                reward = -3\n",
    "        else:\n",
    "            reward = -3\n",
    "        \n",
    "        # add reward for bingo\n",
    "        reward += self.checkFull(0,3,0,3)\n",
    "        reward += self.checkFull(0,3,3,6)\n",
    "        reward += self.checkFull(0,3,6,9)\n",
    "        reward += self.checkFull(3,6,0,3)\n",
    "        reward += self.checkFull(3,6,3,6)\n",
    "        reward += self.checkFull(3,6,6,9)\n",
    "        reward += self.checkFull(6,9,0,3)\n",
    "        reward += self.checkFull(6,9,3,6)\n",
    "        reward += self.checkFull(6,9,6,9)\n",
    "        \n",
    "        # update shapes available\n",
    "        if self.state['items'][0] == 0 and self.state['items'][0] == 0 and self.state['items'][0] == 0:\n",
    "            self.state['items'][0] = random.randint(1,self.number_of_shapes)\n",
    "            self.state['items'][1] = random.randint(1,self.number_of_shapes)\n",
    "            self.state['items'][2] = random.randint(1,self.number_of_shapes)\n",
    "            \n",
    "        # check there are options            \n",
    "        done = not self.thereAreOptions()\n",
    "        if done:\n",
    "            print('shit')\n",
    "        info = {}\n",
    "        flatState = np.zeros(84)\n",
    "        idx = 0\n",
    "        for line in self.state['board']:\n",
    "            for el in line:\n",
    "                flatState[idx] = el\n",
    "                idx += 1\n",
    "        flatState[81] = self.state['items'][0] / self.number_of_shapes\n",
    "        flatState[82] = self.state['items'][1] / self.number_of_shapes\n",
    "        flatState[83] = self.state['items'][2] / self.number_of_shapes\n",
    "        \n",
    "        return flatState, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.state['board'] = np.zeros((9, 9), dtype = np.uint8)\n",
    "        self.state['items'] = np.zeros(3, dtype = np.uint8)\n",
    "        self.state['items'][0] = random.randint(1, self.number_of_shapes)\n",
    "        self.state['items'][1] = random.randint(1, self.number_of_shapes)\n",
    "        self.state['items'][2] = random.randint(1, self.number_of_shapes)\n",
    "        flatState = np.zeros(84)\n",
    "        idx = 0\n",
    "        for line in self.state['board']:\n",
    "            for el in line:\n",
    "                flatState[idx] = el\n",
    "                idx += 1\n",
    "        flatState[81] = self.state['items'][0] / self.number_of_shapes\n",
    "        flatState[82] = self.state['items'][1] / self.number_of_shapes\n",
    "        flatState[83] = self.state['items'][2] / self.number_of_shapes\n",
    "        return flatState\n",
    "    \n",
    "    def render(self, mode = 'shit'):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864b77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SquaresEnv()\n",
    "# episodes = 10\n",
    "# for episode in range(1, episodes + 1):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     score = 0\n",
    "    \n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = env.action_space.sample()\n",
    "#         #print(action)\n",
    "#         n_state, reward, done, info = env.step(action)\n",
    "#         if n_state is None:\n",
    "#             print('fuck')\n",
    "#         score+=reward\n",
    "#     print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b68ee7",
   "metadata": {},
   "source": [
    "# Create a deep learning model with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f324ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "\n",
    "nb_actions = env.action_space.shape[0]\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(128))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(64))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('linear'))\n",
    "#print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = Concatenate()([action_input, flattened_observation])\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(32)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "#print(critic.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce7ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   46/10000 [..............................] - ETA: 22s - reward: -2.7391"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oleg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1096/10000 [==>...........................] - ETA: 44s - reward: -2.8741shit\n",
      " 5493/10000 [===============>..............] - ETA: 23s - reward: -2.9345yay\n",
      " 5502/10000 [===============>..............] - ETA: 23s - reward: -2.9306yay\n",
      "10000/10000 [==============================] - 52s 5ms/step - reward: -2.9446\n",
      "10 episodes - episode_reward: -2676.100 [-2988.000, -255.000] - loss: 7.802 - mae: 0.692 - mean_q: -11.413\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 54s 5ms/step - reward: -2.9922\n",
      "10 episodes - episode_reward: -2992.200 [-3000.000, -2976.000] - loss: 4.785 - mae: 1.671 - mean_q: -19.498\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 53s 5ms/step - reward: -2.9910\n",
      "10 episodes - episode_reward: -2991.600 [-2994.000, -2982.000] - loss: 1.267 - mae: 0.939 - mean_q: -20.138\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -2.9802\n",
      "10 episodes - episode_reward: -2982.000 [-2988.000, -2970.000] - loss: 2.842 - mae: 1.255 - mean_q: -40.549\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      " 4104/10000 [===========>..................] - ETA: 33s - reward: -2.9795yay\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -2.9750\n",
      "10 episodes - episode_reward: -2975.000 [-2988.000, -2960.000] - loss: 7.372 - mae: 1.742 - mean_q: -78.709\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -2.9844\n",
      "10 episodes - episode_reward: -2982.600 [-2994.000, -2964.000] - loss: 13.233 - mae: 2.237 - mean_q: -111.712\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -2.9886\n",
      "10 episodes - episode_reward: -2988.000 [-2994.000, -2976.000] - loss: 20.767 - mae: 2.712 - mean_q: -135.132\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -2.9940\n",
      "10 episodes - episode_reward: -2994.000 [-2994.000, -2994.000] - loss: 32.858 - mae: 3.570 - mean_q: -150.834\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 60s 6ms/step - reward: -2.9928\n",
      "10 episodes - episode_reward: -2992.800 [-2994.000, -2988.000] - loss: 38.999 - mae: 4.124 - mean_q: -168.246\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -2.9886\n",
      "10 episodes - episode_reward: -2988.600 [-2994.000, -2970.000] - loss: 42.881 - mae: 4.150 - mean_q: -182.957\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      " 1180/10000 [==>...........................] - ETA: 52s - reward: -2.9847yay\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -2.9882\n",
      "10 episodes - episode_reward: -2988.200 [-2994.000, -2966.000] - loss: 43.966 - mae: 4.057 - mean_q: -197.642\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9874\n",
      "10 episodes - episode_reward: -2987.400 [-2994.000, -2958.000] - loss: 42.569 - mae: 3.975 - mean_q: -212.567\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9916\n",
      "10 episodes - episode_reward: -2994.000 [-2994.000, -2994.000] - loss: 44.732 - mae: 4.058 - mean_q: -222.082\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -2.9904\n",
      "10 episodes - episode_reward: -2988.000 [-2994.000, -2964.000] - loss: 45.471 - mae: 4.172 - mean_q: -222.877\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -2.9940\n",
      "10 episodes - episode_reward: -2994.000 [-2994.000, -2994.000] - loss: 47.538 - mae: 4.327 - mean_q: -232.270\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9940\n",
      "10 episodes - episode_reward: -2994.000 [-2994.000, -2994.000] - loss: 49.199 - mae: 4.607 - mean_q: -238.454\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9940\n",
      "10 episodes - episode_reward: -2994.000 [-2994.000, -2994.000] - loss: 54.043 - mae: 4.962 - mean_q: -244.595\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -2.9940\n",
      "10 episodes - episode_reward: -2994.000 [-2994.000, -2994.000] - loss: 52.993 - mae: 4.820 - mean_q: -249.336\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -2.9934\n",
      "10 episodes - episode_reward: -2993.400 [-2994.000, -2988.000] - loss: 47.255 - mae: 4.103 - mean_q: -254.866\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 57s 6ms/step - reward: -2.9994\n",
      "10 episodes - episode_reward: -2998.800 [-3000.000, -2994.000] - loss: 44.047 - mae: 3.781 - mean_q: -260.533\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -2.9964\n",
      "10 episodes - episode_reward: -2997.000 [-3000.000, -2994.000] - loss: 49.634 - mae: 4.250 - mean_q: -256.297\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9928\n",
      "10 episodes - episode_reward: -2993.400 [-2994.000, -2988.000] - loss: 48.500 - mae: 4.115 - mean_q: -254.544\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      " 7225/10000 [====================>.........] - ETA: 16s - reward: -2.9942yay\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -2.9906\n",
      "10 episodes - episode_reward: -2990.600 [-3000.000, -2960.000] - loss: 31.319 - mae: 3.456 - mean_q: -197.276\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      " 2132/10000 [=====>........................] - ETA: 50s - reward: -2.9916yay\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9864\n",
      "10 episodes - episode_reward: -2985.800 [-2994.000, -2966.000] - loss: 27.314 - mae: 2.868 - mean_q: -200.331\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -2.9898\n",
      "10 episodes - episode_reward: -2989.800 [-2994.000, -2982.000] - loss: 32.513 - mae: 2.796 - mean_q: -214.766\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -2.9916\n",
      "10 episodes - episode_reward: -2992.200 [-2994.000, -2988.000] - loss: 32.300 - mae: 2.568 - mean_q: -228.987\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -2.9898\n",
      "10 episodes - episode_reward: -2989.800 [-2994.000, -2982.000] - loss: 32.394 - mae: 2.434 - mean_q: -239.950\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -2.9874\n",
      "10 episodes - episode_reward: -2986.800 [-2994.000, -2976.000] - loss: 40.853 - mae: 3.161 - mean_q: -243.113\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9928\n",
      "10 episodes - episode_reward: -2992.800 [-2994.000, -2982.000] - loss: 50.325 - mae: 4.155 - mean_q: -238.829\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -2.9940\n",
      "10 episodes - episode_reward: -2994.000 [-2994.000, -2994.000] - loss: 56.104 - mae: 4.616 - mean_q: -239.518\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9934\n",
      "10 episodes - episode_reward: -2993.400 [-2994.000, -2988.000] - loss: 52.944 - mae: 4.637 - mean_q: -244.157\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -2.9904\n",
      "10 episodes - episode_reward: -2991.000 [-2994.000, -2982.000] - loss: 56.170 - mae: 4.734 - mean_q: -249.218\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 59s 6ms/step - reward: -2.9892\n",
      "10 episodes - episode_reward: -2989.200 [-2994.000, -2982.000] - loss: 50.808 - mae: 4.273 - mean_q: -248.192\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 58s 6ms/step - reward: -2.9964\n",
      "10 episodes - episode_reward: -2995.200 [-3000.000, -2982.000] - loss: 49.900 - mae: 4.074 - mean_q: -244.493\n",
      "\n",
      "Interval 35 (340000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 64s 6ms/step - reward: -2.9928\n",
      "10 episodes - episode_reward: -2993.400 [-3000.000, -2982.000] - loss: 45.611 - mae: 3.774 - mean_q: -237.910\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9904\n",
      "10 episodes - episode_reward: -2990.400 [-2994.000, -2988.000] - loss: 41.322 - mae: 3.753 - mean_q: -230.168\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      " 8110/10000 [=======================>......] - ETA: 12s - reward: -2.9889yay\n",
      " 9192/10000 [==========================>...] - ETA: 5s - reward: -2.9865yay\n",
      "10000/10000 [==============================] - 64s 6ms/step - reward: -2.9848\n",
      "10 episodes - episode_reward: -2988.200 [-2994.000, -2970.000] - loss: 44.488 - mae: 3.863 - mean_q: -229.114\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      " 3113/10000 [========>.....................] - ETA: 42s - reward: -2.9884yay\n",
      " 6114/10000 [=================>............] - ETA: 24s - reward: -2.9846yay\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9860\n",
      "10 episodes - episode_reward: -2982.600 [-2994.000, -2960.000] - loss: 36.102 - mae: 3.257 - mean_q: -231.739\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      " 6160/10000 [=================>............] - ETA: 23s - reward: -2.9893yay\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9882\n",
      "10 episodes - episode_reward: -2988.800 [-2994.000, -2966.000] - loss: 33.926 - mae: 2.565 - mean_q: -242.512\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 61s 6ms/step - reward: -2.9874\n",
      "10 episodes - episode_reward: -2987.400 [-2994.000, -2970.000] - loss: 38.375 - mae: 2.540 - mean_q: -252.989\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 63s 6ms/step - reward: -2.9838\n",
      "10 episodes - episode_reward: -2985.000 [-2994.000, -2970.000] - loss: 41.450 - mae: 2.598 - mean_q: -261.613\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "  527/10000 [>.............................] - ETA: 58s - reward: -2.9772done, took 2491.187 seconds\n"
     ]
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15, mu=0., sigma=.3)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=200, nb_steps_warmup_actor=200,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=3e-3)\n",
    "agent.compile(Adam(learning_rate=.003, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "agent.fit(env, nb_steps=1000000, visualize=True, verbose=1, nb_max_episode_steps=1000)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights('ddpg_{}_weights.h5f'.format(\"damn\"), overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91645431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -2988.000, steps: 1000\n",
      "Episode 2: reward: -2988.000, steps: 1000\n",
      "Episode 3: reward: -2988.000, steps: 1000\n",
      "Episode 4: reward: -2994.000, steps: 1000\n",
      "Episode 5: reward: -2988.000, steps: 1000\n",
      "Episode 6: reward: -2988.000, steps: 1000\n",
      "Episode 7: reward: -2994.000, steps: 1000\n",
      "Episode 8: reward: -2988.000, steps: 1000\n",
      "Episode 9: reward: -2994.000, steps: 1000\n",
      "Episode 10: reward: -2994.000, steps: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x240c7cee050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "agent.test(env, nb_episodes=10, visualize=True, nb_max_episode_steps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
